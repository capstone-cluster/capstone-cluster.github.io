<!DOCTYPE html>
<html lang="en">
  <head id="head">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width" />
    <meta name="description" content="Project CLUSTER - Multi-Purpose HPC Computing for the ECE Department">
    <meta name="author" content="Project CLUSTER">
    <title>Project CLUSTER</title>
    <link rel="shortcut icon" href="img/favicon.ico">
    <!-- CSS/JS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.4.1/dist/jquery.min.js,npm/bootstrap@4.3.1/dist/js/bootstrap.min.js"></script>
    <link href="css/style.css" rel="stylesheet" type="text/css">
  </head>

  <body class="bg-dark" data-spy="scroll" data-target=".navbar" data-offset="75">
    <header>
      <nav class="navbar fixed-top nav-pills navbar-expand-lg navbar-dark bg-dark">
        <a class="navbar-brand" href="/index.html#head"><i class="fa fa-server"></i> Project CLUSTER</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <div class="navbar-nav ml-auto">
            <a class="nav-item nav-link" href="/index.html#about">About</a>
            <a class="nav-item nav-link" href="/index.html#team">Team</a>
            <a class="nav-item nav-link" href="/index.html#contact">Contact</a>
          </div>
        </div>
      </nav>
    </header>
    <main role="main">
      <section class="jumbotron text-center" id="blog-top">
        <div class="container-fluid">
          <h1 class="jumbotron-heading">Adam's Blog</h1>
          <hr />
          <p>
            A journal of everything I've done for Project CLUSTER
          </p>
        </div>
      </section>
      <section id="blog-section">
        <br><br>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of March 9th</h2>
                <p class="blog-post-meta">March 12, 2020 by Adam Dadey</p>
                <p>
                  This week I worked out some bugs with the cluster configuration
                  script I started drafting a few weeks ago.  The script now correctly
                  downloads, installs, and configures MUNGE, slurm, and Open MPI.  Dr.
                  Cotton is making progress with getting the C7000s up.  When the blades
                  are up I will be ready to start implementing the supercomputer on the
                  blades.  Additonally, all classes are supposed to be online due to
                  Coronavirus fears.  Hopefully this does not impede the progress of
                  this project.
                </p>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of March 2nd</h2>
                <p class="blog-post-meta">March 5, 2020 by Adam Dadey</p>
                <p>
                  This week I was visiting UT Austin as a potential graduate school
                  on Thursday and Friday so I did not do any work on the project.
                  Dr. Cotton started fixing up the C7000 though, so I will hopefully
                  be able to start moving slurm to an actual cluster next week.
                </p>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of February 24th</h2>
                <p class="blog-post-meta">February 27, 2020 by Adam Dadey</p>
                <p>
                  This week I drafted a configuration script that will install and
                  configure all required software for each node on the cluster.  In
                  its current state, the admin will still have to do a few extra steps
                  after running the script to fully configure a node.  I plan to make
                  the script do all the configuration in the future.  Additionally, I
                  ran a prime number finding mpi program in the cluster and compared
                  calculation times for one node and three nodes.  Not suprisingly,
                  three nodes were able to finish the calculation quicker than one node.
                  The results for one node and three nodes are below on the left and
                  right respectively.
                </p>
                <a href="/img/blog-adam/slurm-prime-mpi-results.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/slurm-prime-mpi-results.png" style="max-width: 70%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of February 17th</h2>
                <p class="blog-post-meta">February 20, 2020 by Adam Dadey</p>
                <p>
                  This week I got the sbcast command to work with slurm.  The sbcast
                  command is supposed to be used to disperse a file to all the nodes
                  allocated to a specific job.  Unfortunately, the command was throwing
                  an error saying it could not find the given file.  The problem was that
                  the sbcast command was being run on the first node allocated to the job.
                  I initialized the job on the master, so the sbcast command was being run
                  on the first worker.  The file I was trying to disperse was not on the
                  worker node so it was throwing an error.  The solution is to run jobs from
                  the worker.  With this change the command worked!
                </p>
                <a href="/img/blog-adam/slurm-sbcast-script.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/slurm-sbcast-script.png" style="max-width: 65%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of February 10th</h2>
                <p class="blog-post-meta">February 13, 2020 by Adam Dadey</p>
                <p>
                  This week was the first week of Spring Semester. I installed Open MPI,
                  which allows for parallel processing in the cluster. Additionally,
                  I adjusted the slurm.conf file so that system memory is an allocatable
                  resource. I researched OpenHPC, a set of tools for creating a cluster. One
                  of the guides describes using CentOS 7 as the base os, Warewulf to install
                  and manage the cluster, and slurm as the job scheduler for the cluster. This
                  configuration is very similar to what I currently have implemented on VMs.
                  I could potentially use the OpenHPC resources for implementing the cluster
                  on the C7000 blades. Finally, I've been reading up on slurm documentation
                  to get a better grasp of its functionality. Below is a image of a submit
                  script that demonstrates memory allocation and running an MPI program.
                </p>
                <a href="/img/blog-adam/slurm-sbatch-script-mpi.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/slurm-sbatch-script-mpi.png" style="max-width: 65%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of November 25th (Thanksgiving Break)</h2>
                <p class="blog-post-meta">November 30, 2019 by Adam Dadey</p>
                <p>
                  This week I worked on the Mid-Year Presentation. Our presentation is either
                  December 3rd or 5th so I wanted to get a head start on formatting and
                  filling in the presentation.
                </p>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of November 19th</h2>
                <p class="blog-post-meta">November 21, 2019 by Adam Dadey</p>
                <p>
                  This week I worked out some bugs with slurm.  I found that over time
                  the clocks across the cluster would become out of sync.  The MUNGE
                  authenticator needs in sync clocks.  I created bash script to run on
                  each node that syncs the node with an NTP server.  Additionally, I set up
                  logging for running and completed jobs.  Below is a screenshot of the
                  completed jobs log filled with a few completed jobs.
                </p>
                <a href="/img/blog-adam/slurm-master-job-log.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/slurm-master-job-log.png" style="max-width: 65%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of November 11th</h2>
                <p class="blog-post-meta">November 14, 2019 by Adam Dadey</p>
                <p>
                  This week I was able to create a head node and worker node and get them to
                  communicated with eachother.  Unfortunately, while trying to install slurm
                  I encountered an error during the source build because it could not find python.
                  In CentOS 8, the version I was using, there is no default python environment, which
                  the builder relies on.  To fix this problem I moved to CentOS 7 and was successfully
                  able to build and install slurm on both the head node and worker node.  Below is an
                  image of the 'scontrol show nodes' command which properly shows the worker node.
                </p>
                <a href="/img/blog-adam/slurm-master-found-node.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/slurm-master-found-node.png" style="max-width: 65%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of November 4th</h2>
                <p class="blog-post-meta">November 7, 2019 by Adam Dadey</p>
                <p>
                  This week I started exploring slurm, an open source job scheduler commonly used
                  in supercomputers.  We plan to use slurm as the backbone of our cluster supercomputer.
                  Red Hat is a common linux distro for cluster nodes, so I am experimenting with CentOS, a
                  free version of Red Hat, as the barebones OS for each node on the cluster.  In VMWare, I
                  started configuring a test master and worker node by first installing MUNGE.  MUNGE is an
                  authentication service used to create and validate credentials, and it is used by slurm.
                </p>
                <a href="/img/blog-adam/centos.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/centos.png" style="max-width: 65%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of October 28th</h2>
                <p class="blog-post-meta">October 31, 2019 by Adam Dadey</p>
                <p>
                  This week I got an email from UD IT about a vulnerability,
                  CVE-2019-5119, in our server. This vulnerability deals with
                  the version of ESXi we are running.  The solution was the update
                  ESXi to the latest patch.  I was able to SSH into the server,
                  patch the software, and remove the vulnerability.  Below is a
                  description of the CVE.
                </p>
                <a href="/img/blog-adam/ESXi_CVE.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/ESXi_CVE.png" style="max-width: 85%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of October 21st</h2>
                <p class="blog-post-meta">October 25, 2019 by Adam Dadey</p>
                <p>
                  This week I tried to solve the problem with Alpine Linux from
                  last week.  This effort was unsuccessful.  Although, we found
                  that Rancher orchestration uses RancherOS as the base OS and
                  we had success spawning nodes.  We have decided to no longer
                  pursue Alpine Linux as a base OS for each node of the cluster.
                </p>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of October 14th</h2>
                <p class="blog-post-meta">October 18, 2019 by Adam Dadey</p>
                <p>
                  This week I experimented with light weight linux distros to use as a baremetal
                  operating system for each worker computer in the cluster.  I found that Alpine
                  Linux is a popular light weight distro for cluster computing.  I was able to
                  successfully install the OS on ESXi and install Docker.  Although, there is
                  currently a problem getting the worker to communicate with the cluster.  I hope
                  to fix this problem next week.  Below is an image of Alpine running in ESXi.
                </p>
                <a href="/img/blog-adam/alpine_splash.PNG" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/alpine_splash.PNG" style="max-width: 60%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of October 7th</h2>
                <p class="blog-post-meta">October 11, 2019 by Adam Dadey</p>
                <p>
                  This week I setup the DRAC5 on the Dell R805 server.  I found that Internet Explorer
                  is required to use the server console view and insert virtual media.  Although,
                  browsers like Firefox or Chrome can be used to view the web based splash screen for
                  the server. Below is an image of the DRAC5 main screen.
                </p>
                <a href="/img/blog-adam/DRAC_splash_screen.PNG" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/DRAC_splash_screen.PNG" style="max-width: 85%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid">
          <div class="row justify-content-center">
            <div class="col-10 col-md-8">
              <div class="blog-post">
                <h2 class="blog-post-title">Week of September 30th</h2>
                <p class="blog-post-meta">October 4, 2019 by Adam Dadey</p>
                <p>
                  This week I installed and configured RancherOS in VMWare Workstation.  We plan to
                  use Rancher to manage our Kubernetes cluster.  RancherOS is a containerized version
                  of Linux designed to be a lightweight baremetal OS for Rancher. I was able to create
                  and link three RancherOS VMs, one for the main controller and two as workers.  The
                  three VMs were able to successfully work with eachother.  Below is an image of
                  RancherOS running in a VM.
                </p>
                <a href="/img/blog-adam/RancherOS_splash.png" data-lightbox="image">
                  <div class="justify-content-center d-flex">
                    <img class="img-fluid" src="/img/blog-adam/RancherOS_splash.png" style="max-width: 85%; align-self: center; border: 2px solid #000;">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
      </section>
    </main>
    <footer>
      <p class="text-center">
        Copyright &copy;
        <script type="text/JavaScript">document.write(new Date().getFullYear());</script>
        Project CLUSTER, All Rights Reserved
      </p>
    </footer>
    <!-- CSS/JS -->
    <link href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" media="none" onload="if(media!='all')media='all'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"></noscript>
    <script src="js/main.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.1/dist/css/lightbox.min.css" rel="stylesheet" type="text/css">
    <script src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.1/dist/js/lightbox.min.js"></script>
  </body>

</html>
